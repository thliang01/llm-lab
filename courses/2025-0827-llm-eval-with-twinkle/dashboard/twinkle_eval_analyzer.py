import streamlit as st
import json
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, List, Any, Optional
import io

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="âœ¨ Twinkle Eval Analyzer (.json / .jsonl)",
    page_icon="âœ¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾© CSS æ¨£å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f2937;
        text-align: center;
        margin-bottom: 2rem;
        padding: 1rem;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }
    
    .metric-card {
        background-color: #f8fafc;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #3b82f6;
        margin-bottom: 1rem;
    }
    
    .dataset-card {
        background-color: #ffffff;
        padding: 1.5rem;
        border-radius: 0.75rem;
        box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1);
        border: 1px solid #e5e7eb;
        margin-bottom: 1rem;
    }
    
    .stFileUploader {
        border: 2px dashed #cbd5e1;
        border-radius: 0.5rem;
        padding: 2rem;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

def load_json_data(uploaded_file) -> Optional[Dict]:
    """è¼‰å…¥ä¸Šå‚³çš„ JSON æª”æ¡ˆ"""
    try:
        if uploaded_file.name.endswith('.json'):
            content = uploaded_file.read().decode('utf-8')
            return json.loads(content)
        elif uploaded_file.name.endswith('.jsonl'):
            content = uploaded_file.read().decode('utf-8')
            lines = content.strip().split('\n')
            return [json.loads(line) for line in lines if line.strip()]
        else:
            st.error("è«‹ä¸Šå‚³ .json æˆ– .jsonl æª”æ¡ˆ")
            return None
    except Exception as e:
        st.error(f"æª”æ¡ˆè¼‰å…¥éŒ¯èª¤ï¼š{str(e)}")
        return None

def aggregate_multiple_jsonl_files(files_data: List[List[Dict]]) -> Dict:
    """èšåˆå¤šå€‹ JSONL æª”æ¡ˆçš„çµæœ"""
    if not files_data:
        return {}
    
    # åˆä½µæ‰€æœ‰é¡Œç›®è³‡æ–™ï¼Œä¸¦æ·»åŠ ä¾†æºæª”æ¡ˆè³‡è¨Š
    all_questions = []
    for run_idx, file_data in enumerate(files_data):
        for question in file_data:
            question_copy = question.copy()
            question_copy['source_run'] = run_idx  # æ·»åŠ ä¾†æºè¼ªæ¬¡æ¨™è¨˜
            all_questions.append(question_copy)
    
    # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
    total_questions = len(all_questions)
    correct_questions = sum(1 for q in all_questions if q.get('is_correct', False))
    overall_accuracy = (correct_questions / total_questions) * 100 if total_questions > 0 else 0
    
    # æŒ‰é¡Œç›® ID åˆ†çµ„ä»¥è¨ˆç®—æ¯é¡Œçš„æº–ç¢ºç‡
    question_groups = {}
    for question in all_questions:
        q_id = question.get('question_id', 0)
        if q_id not in question_groups:
            question_groups[q_id] = []
        question_groups[q_id].append(question)
    
    # è¨ˆç®—æ¯é¡Œçš„çµ±è¨ˆè³‡è¨Š
    aggregated_questions = []
    for q_id, questions in question_groups.items():
        correct_count = sum(1 for q in questions if q.get('is_correct', False))
        total_runs = len(questions)
        accuracy = (correct_count / total_runs) * 100 if total_runs > 0 else 0
        
        # å–ç¬¬ä¸€å€‹å•é¡Œä½œç‚ºåŸºç¤è³‡æ–™
        base_question = questions[0].copy()
        base_question.update({
            'run_count': total_runs,
            'correct_count': correct_count,
            'run_accuracy': accuracy,
            'all_runs_data': questions  # ä¿å­˜æ‰€æœ‰è¼ªæ¬¡çš„è³‡æ–™
        })
        aggregated_questions.append(base_question)
    
    return {
        'individual_questions': all_questions,
        'aggregated_questions': aggregated_questions,
        'total_questions': len(question_groups),
        'total_runs': len(files_data),
        'overall_accuracy': overall_accuracy,
        'total_evaluations': total_questions
    }

def parse_evaluation_results(data: Dict) -> Dict:
    """è§£æè©•æ¸¬çµæœè³‡æ–™"""
    if not data:
        return {}
    
    # è™•ç† Twinkle Eval çµæœæ ¼å¼
    if 'dataset_results' in data:
        results = []
        for dataset_path, dataset_info in data['dataset_results'].items():
            if 'results' in dataset_info and dataset_info['results']:
                for result in dataset_info['results']:
                    results.append({
                        'dataset_path': dataset_path,
                        'file': result.get('file', ''),
                        'accuracy_mean': result.get('accuracy_mean', 0),
                        'accuracy_std': result.get('accuracy_std', 0),
                        'individual_runs': result.get('individual_runs', {})
                    })
        return {
            'results': results,
            'config': data.get('config', {}),
            'timestamp': data.get('timestamp', ''),
            'duration_seconds': data.get('duration_seconds', 0)
        }
    
    # è™•ç† JSONL æ ¼å¼ï¼ˆå€‹åˆ¥é¡Œç›®çµæœï¼‰
    if isinstance(data, list):
        return {'individual_questions': data}
    
    return data

def create_accuracy_chart(results: List[Dict]) -> go.Figure:
    """å»ºç«‹æº–ç¢ºç‡åœ–è¡¨"""
    if not results:
        return go.Figure()
    
    df = pd.DataFrame(results)
    
    fig = px.bar(
        df,
        x='file',
        y='accuracy_mean',
        error_y='accuracy_std',
        title='å„è³‡æ–™é›†æº–ç¢ºç‡è¡¨ç¾',
        labels={'accuracy_mean': 'å¹³å‡æº–ç¢ºç‡', 'file': 'è³‡æ–™é›†æª”æ¡ˆ'},
        color='accuracy_mean',
        color_continuous_scale='viridis'
    )
    
    fig.update_layout(
        xaxis_tickangle=-45,
        height=500,
        showlegend=False
    )
    
    return fig

def create_runs_comparison_chart(result: Dict) -> go.Figure:
    """å»ºç«‹å¤šè¼ªè©•æ¸¬æ¯”è¼ƒåœ–è¡¨"""
    if 'individual_runs' not in result or 'accuracies' not in result['individual_runs']:
        return go.Figure()
    
    accuracies = result['individual_runs']['accuracies']
    run_numbers = list(range(1, len(accuracies) + 1))
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(
        x=run_numbers,
        y=accuracies,
        mode='lines+markers',
        name='æº–ç¢ºç‡',
        line=dict(color='#3b82f6', width=3),
        marker=dict(size=8, color='#1e40af')
    ))
    
    # æ·»åŠ å¹³å‡ç·š
    mean_accuracy = sum(accuracies) / len(accuracies)
    fig.add_hline(
        y=mean_accuracy,
        line_dash="dash",
        line_color="red",
        annotation_text=f"å¹³å‡: {mean_accuracy:.3f}"
    )
    
    fig.update_layout(
        title=f'å¤šè¼ªè©•æ¸¬çµæœæ¯”è¼ƒ ({result["file"]})',
        xaxis_title='è©•æ¸¬è¼ªæ¬¡',
        yaxis_title='æº–ç¢ºç‡',
        height=400
    )
    
    return fig

def display_individual_questions(questions: List[Dict], score_filter: tuple, items_per_page: int, sort_by: str, is_aggregated: bool = False, section_key: str = "default"):
    """é¡¯ç¤ºå€‹åˆ¥é¡Œç›®çµæœ"""
    if not questions:
        st.info("æ²’æœ‰å€‹åˆ¥é¡Œç›®è³‡æ–™å¯é¡¯ç¤º")
        return
    
    # éæ¿¾è³‡æ–™
    if is_aggregated:
        # èšåˆè³‡æ–™ä½¿ç”¨ run_accuracy é€²è¡Œéæ¿¾
        if score_filter[0] == 0 and score_filter[1] == 100:
            filtered_questions = questions
        else:
            filtered_questions = [
                q for q in questions 
                if score_filter[0] <= q.get('run_accuracy', 0) <= score_filter[1]
            ]
    else:
        # å–®æ¬¡çµæœè³‡æ–™ä½¿ç”¨ is_correct é€²è¡Œéæ¿¾
        if 'is_correct' in questions[0]:
            if score_filter[0] == 0 and score_filter[1] == 100:
                filtered_questions = questions
            else:
                # å°‡ is_correct è½‰æ›ç‚ºåˆ†æ•¸ (True=100, False=0)
                filtered_questions = [
                    q for q in questions 
                    if score_filter[0] <= (100 if q.get('is_correct', False) else 0) <= score_filter[1]
                ]
        else:
            filtered_questions = questions
    
    # æ’åº
    if is_aggregated:
        if sort_by == "æº–ç¢ºç‡ç”±ä½åˆ°é«˜":
            filtered_questions.sort(key=lambda x: x.get('run_accuracy', 0))
        elif sort_by == "æº–ç¢ºç‡ç”±é«˜åˆ°ä½":
            filtered_questions.sort(key=lambda x: x.get('run_accuracy', 0), reverse=True)
        elif sort_by == "é¡Œç›® ID":
            filtered_questions.sort(key=lambda x: x.get('question_id', 0))
    else:
        if sort_by == "æº–ç¢ºç‡ç”±ä½åˆ°é«˜":
            filtered_questions.sort(key=lambda x: x.get('is_correct', False))
        elif sort_by == "æº–ç¢ºç‡ç”±é«˜åˆ°ä½":
            filtered_questions.sort(key=lambda x: x.get('is_correct', False), reverse=True)
        elif sort_by == "é¡Œç›® ID":
            filtered_questions.sort(key=lambda x: x.get('question_id', 0))
    
    # åˆ†é 
    total_items = len(filtered_questions)
    total_pages = (total_items - 1) // items_per_page + 1 if total_items > 0 else 1
    
    # é¡¯ç¤ºéæ¿¾å¾Œçš„çµ±è¨ˆ
    if section_key == "individual" and len(questions) > 0:
        original_count = len(questions)
        filtered_correct = sum(1 for q in filtered_questions if q.get('is_correct', False))
        filtered_incorrect = total_items - filtered_correct
        if total_items != original_count:
            st.warning(f"âš ï¸ æ ¹æ“šåˆ†æ•¸ç¯„åœ {score_filter[0]}-{score_filter[1]}% éæ¿¾å¾Œï¼Œé¡¯ç¤º {total_items}/{original_count} å€‹çµæœï¼ˆ{filtered_correct} å€‹æ­£ç¢ºï¼Œ{filtered_incorrect} å€‹éŒ¯èª¤ï¼‰")
        else:
            st.info(f"ğŸ“Š å…± {total_items} å€‹çµæœï¼ˆ{filtered_correct} å€‹æ­£ç¢ºï¼Œ{filtered_incorrect} å€‹éŒ¯èª¤ï¼‰")
    
    page_key = f"page_{section_key}"
    if page_key not in st.session_state:
        st.session_state[page_key] = 1
    
    # åˆ†é æ§åˆ¶
    col1, col2, col3 = st.columns([1, 2, 1])
    with col1:
        if st.button("ä¸Šä¸€é ", disabled=st.session_state[page_key] <= 1, key=f"prev_{section_key}"):
            st.session_state[page_key] -= 1
            st.rerun()
    
    with col2:
        st.write(f"ç¬¬ {st.session_state[page_key]} é ï¼Œå…± {total_pages} é  (ç¸½å…± {total_items} é¡Œ)")
    
    with col3:
        if st.button("ä¸‹ä¸€é ", disabled=st.session_state[page_key] >= total_pages, key=f"next_{section_key}"):
            st.session_state[page_key] += 1
            st.rerun()
    
    # é¡¯ç¤ºç•¶å‰é é¢çš„é¡Œç›®
    start_idx = (st.session_state[page_key] - 1) * items_per_page
    end_idx = min(start_idx + items_per_page, total_items)
    current_questions = filtered_questions[start_idx:end_idx]
    
    for i, question in enumerate(current_questions, start=start_idx + 1):
        if is_aggregated:
            # èšåˆè³‡æ–™é¡¯ç¤º
            accuracy_status = f"{question.get('run_accuracy', 0):.1f}% ({question.get('correct_count', 0)}/{question.get('run_count', 0)} è¼ªæ­£ç¢º)"
            with st.expander(f"é¡Œç›® {question.get('question_id', i)} - {accuracy_status}"):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown("**é¡Œç›®å…§å®¹ï¼š**")
                    st.write(question.get('question', 'N/A'))
                    
                    st.markdown("**æ­£ç¢ºç­”æ¡ˆï¼š**")
                    st.write(question.get('correct_answer', 'N/A'))
                    
                    # é¡¯ç¤ºå„è¼ªæ¬¡çš„è¼¸å‡ºçµæœ
                    if 'all_runs_data' in question:
                        st.markdown("**å„è¼ªæ¬¡æ¨¡å‹è¼¸å‡ºï¼š**")
                        for run_idx, run_data in enumerate(question['all_runs_data']):
                            is_correct = "âœ…" if run_data.get('is_correct') else "âŒ"
                            with st.expander(f"ç¬¬ {run_idx + 1} è¼ª {is_correct}", expanded=False):
                                st.write(f"**è¼¸å‡ºï¼š** {run_data.get('llm_output', 'N/A')}")
                                if run_data.get('predicted_answer'):
                                    st.write(f"**è§£æç­”æ¡ˆï¼š** {run_data.get('predicted_answer', 'N/A')}")
                
                with col2:
                    st.markdown("**èšåˆçµ±è¨ˆï¼š**")
                    st.metric("è©•æ¸¬è¼ªæ¬¡", question.get('run_count', 0))
                    st.metric("æ­£ç¢ºæ¬¡æ•¸", question.get('correct_count', 0))
                    st.metric("æº–ç¢ºç‡", f"{question.get('run_accuracy', 0):.1f}%")
                    
                    # é¡¯ç¤º Token ä½¿ç”¨é‡ï¼ˆå–å¹³å‡å€¼ï¼‰
                    if 'all_runs_data' in question:
                        all_runs = question['all_runs_data']
                        if all_runs and 'usage_total_tokens' in all_runs[0]:
                            avg_total_tokens = sum(run.get('usage_total_tokens', 0) for run in all_runs) / len(all_runs)
                            st.metric("å¹³å‡ç¸½ Token", f"{avg_total_tokens:.0f}")
        else:
            # å–®æ¬¡çµæœé¡¯ç¤º
            correct_status = question.get('is_correct', False)
            status_text = 'âœ… æ­£ç¢º' if correct_status else 'âŒ éŒ¯èª¤'
            
            # å¦‚æœæ˜¯ä¾†è‡ªå¤šæª”æ¡ˆèšåˆï¼Œé¡¯ç¤ºæ›´è©³ç´°çš„æ¨™é¡Œ
            if section_key == "individual" and 'source_run' in question:
                title = f"é¡Œç›® {question.get('question_id', i)} (ç¬¬ {question['source_run'] + 1} è¼ª) - {status_text}"
            else:
                title = f"é¡Œç›® {question.get('question_id', i)} - {status_text}"
            
            with st.expander(title):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown("**é¡Œç›®å…§å®¹ï¼š**")
                    st.write(question.get('question', 'N/A'))
                    
                    st.markdown("**æ­£ç¢ºç­”æ¡ˆï¼š**")
                    st.write(question.get('correct_answer', 'N/A'))
                    
                    st.markdown("**æ¨¡å‹è¼¸å‡ºï¼š**")
                    st.write(question.get('llm_output', 'N/A'))
                    
                    if question.get('predicted_answer'):
                        st.markdown("**è§£æå¾Œç­”æ¡ˆï¼š**")
                        st.write(question.get('predicted_answer', 'N/A'))
                
                with col2:
                    st.markdown("**çµ±è¨ˆè³‡è¨Šï¼š**")
                    st.metric("ç­”æ¡ˆæ­£ç¢ºæ€§", "æ­£ç¢º" if correct_status else "éŒ¯èª¤")
                    if 'source_run' in question:
                        st.metric("è©•æ¸¬è¼ªæ¬¡", f"ç¬¬ {question['source_run'] + 1} è¼ª")
                    if 'usage_total_tokens' in question:
                        st.metric("ç¸½ Token æ•¸", question['usage_total_tokens'])
                    if 'usage_prompt_tokens' in question:
                        st.metric("æç¤º Token", question['usage_prompt_tokens'])
                    if 'usage_completion_tokens' in question:
                        st.metric("å®Œæˆ Token", question['usage_completion_tokens'])

def main():
    # æ¨™é¡Œå€åŸŸåŒ…å« Logo
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        # é¡¯ç¤º Twinkle Logo
        try:
            st.image("../assets/twinkle_AI_llm_lab.png", width=200)
        except:
            # å¦‚æœæ‰¾ä¸åˆ° logoï¼Œé¡¯ç¤º emoji
            st.markdown('<div style="text-align: center; font-size: 4rem;">âœ¨</div>', unsafe_allow_html=True)
        
        # ä¸»æ¨™é¡Œ
        st.markdown('<h1 class="main-header">Twinkle Eval Analyzer</h1>', unsafe_allow_html=True)
        st.markdown('<p style="text-align: center; color: #666; margin-bottom: 2rem;">åˆ†æ .json / .jsonl æ ¼å¼çš„è©•æ¸¬çµæœ</p>', unsafe_allow_html=True)
    
    # å´é‚Šæ¬„ï¼šæª”æ¡ˆä¸Šå‚³å’Œè¨­å®š
    with st.sidebar:
        st.header("é¸æ“‡ Twinkle Eval æª”æ¡ˆ")
        
        uploaded_files = st.file_uploader(
            "è«‹ä¸Šå‚³ Twinkle Eval æª”æ¡ˆ",
            type=['json', 'jsonl'],
            help="æ”¯æ´æ ¼å¼ï¼š.json (è©•æ¸¬çµæœ) æˆ– .jsonl (å€‹åˆ¥é¡Œç›®çµæœ)\næª”æ¡ˆå¤§å°é™åˆ¶ï¼š200MB\nå¯åŒæ™‚ä¸Šå‚³å¤šå€‹ JSONL æª”æ¡ˆé€²è¡Œèšåˆåˆ†æ",
            accept_multiple_files=True
        )
        
        st.markdown("---")
        
        # éæ¿¾è¨­å®š
        st.header("é¡¯ç¤ºè¨­å®š")
        
        score_filter = st.slider(
            "åˆ†æ•¸ç¯„åœéæ¿¾",
            min_value=0,
            max_value=100,
            value=(0, 100),
            step=1,
            help="ä»¥ 0-100 é¡¯ç¤ºé¡Œç›®åˆ†æ•¸ç¯„åœ"
        )
        
        items_per_page = st.selectbox(
            "æ¯é é¡¯ç¤ºå¹¾å€‹é …ç›®",
            options=[10, 20, 50, 100],
            index=1,
            help="é¸æ“‡æ¯é é¡¯ç¤ºçš„é …ç›®æ•¸é‡"
        )
        
        sort_by = st.selectbox(
            "æ’åºæ–¹å¼",
            options=["é¡Œç›® ID", "æº–ç¢ºç‡ç”±é«˜åˆ°ä½", "æº–ç¢ºç‡ç”±ä½åˆ°é«˜"],
            index=0,
            help="é¸æ“‡è³‡æ–™æ’åºæ–¹å¼"
        )
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    if uploaded_files:
        # è™•ç†å¤šæª”æ¡ˆä¸Šå‚³
        if len(uploaded_files) == 1:
            # å–®ä¸€æª”æ¡ˆè™•ç†
            with st.spinner("æ­£åœ¨è¼‰å…¥æª”æ¡ˆ..."):
                raw_data = load_json_data(uploaded_files[0])
            
            if raw_data:
                # è§£æè³‡æ–™
                parsed_data = parse_evaluation_results(raw_data)
                uploaded_file = uploaded_files[0]  # ç‚ºäº†å‘ä¸‹ç›¸å®¹
        else:
            # å¤šæª”æ¡ˆè™•ç† - åªæ”¯æ´ JSONL æ ¼å¼
            jsonl_files = [f for f in uploaded_files if f.name.endswith('.jsonl')]
            json_files = [f for f in uploaded_files if f.name.endswith('.json')]
            
            if json_files:
                st.warning("âš ï¸ å¤šæª”æ¡ˆä¸Šå‚³æ¨¡å¼ä¸‹æš«ä¸æ”¯æ´ .json æª”æ¡ˆï¼Œè«‹åªä¸Šå‚³ .jsonl æª”æ¡ˆ")
            
            if jsonl_files:
                with st.spinner(f"æ­£åœ¨è¼‰å…¥ {len(jsonl_files)} å€‹ JSONL æª”æ¡ˆ..."):
                    files_data = []
                    file_names = []
                    
                    for file in jsonl_files:
                        data = load_json_data(file)
                        if data and isinstance(data, list):
                            files_data.append(data)
                            file_names.append(file.name)
                        else:
                            st.error(f"è¼‰å…¥æª”æ¡ˆå¤±æ•—ï¼š{file.name}")
                
                if files_data:
                    # èšåˆå¤šå€‹ JSONL æª”æ¡ˆ
                    parsed_data = aggregate_multiple_jsonl_files(files_data)
                    parsed_data['file_names'] = file_names  # ä¿å­˜æª”æ¡ˆåç¨±è³‡è¨Š
                    raw_data = True  # æ¨™è¨˜è³‡æ–™å·²è¼‰å…¥
                else:
                    raw_data = None
            else:
                st.error("è«‹ä¸Šå‚³è‡³å°‘ä¸€å€‹ .jsonl æª”æ¡ˆ")
                raw_data = None
        
        if raw_data and parsed_data:
            
            # é¡¯ç¤ºåŸºæœ¬è³‡è¨Š
            if 'file_names' in parsed_data:
                # å¤šæª”æ¡ˆèšåˆçµæœ
                st.success(f"âœ… æˆåŠŸèšåˆ {len(parsed_data['file_names'])} å€‹ JSONL æª”æ¡ˆ")
                
                # é¡¯ç¤ºèšåˆçµ±è¨ˆ
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                    st.metric("è¼‰å…¥æª”æ¡ˆæ•¸", len(parsed_data['file_names']))
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with col2:
                    st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                    st.metric("é¡Œç›®ç¸½æ•¸", parsed_data['total_questions'])
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with col3:
                    st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                    st.metric("ç¸½è©•æ¸¬è¼ªæ¬¡", parsed_data['total_runs'])
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with col4:
                    st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                    st.metric("æ•´é«”æº–ç¢ºç‡", f"{parsed_data['overall_accuracy']:.2f}%")
                    st.markdown('</div>', unsafe_allow_html=True)
                
                # é¡¯ç¤ºæª”æ¡ˆæ¸…å–®
                with st.expander("ğŸ“ è¼‰å…¥çš„æª”æ¡ˆæ¸…å–®", expanded=False):
                    for i, filename in enumerate(parsed_data['file_names'], 1):
                        st.write(f"{i}. {filename}")
                
                st.markdown("---")
                
                # é¡¯ç¤ºèšåˆé¡Œç›®çµæœ
                st.header("ğŸ“Š å¤šè¼ªè©•æ¸¬èšåˆçµæœ")
                st.info("ä»¥ä¸‹é¡¯ç¤ºæ¯é¡Œåœ¨å¤šè¼ªè©•æ¸¬ä¸­çš„è¡¨ç¾çµ±è¨ˆ")
                
                # ä¿®æ”¹ display_individual_questions èª¿ç”¨ä»¥ä½¿ç”¨èšåˆè³‡æ–™
                display_individual_questions(
                    parsed_data.get('aggregated_questions', []),
                    score_filter,
                    items_per_page,
                    sort_by,
                    is_aggregated=True,
                    section_key="aggregated"
                )
                
                # ä¹Ÿå¯ä»¥é¡¯ç¤ºæ‰€æœ‰åŸå§‹çµæœ
                if st.checkbox("é¡¯ç¤ºæ‰€æœ‰åŸå§‹è©•æ¸¬çµæœï¼ˆåŒ…å«é‡è¤‡é¡Œç›®ï¼‰", key="show_raw_results"):
                    st.header("ğŸ“ æ‰€æœ‰åŸå§‹è©•æ¸¬çµæœ")
                    
                    # æ·»åŠ èª¿è©¦è³‡è¨Š
                    individual_questions = parsed_data.get('individual_questions', [])
                    if individual_questions:
                        total_count = len(individual_questions)
                        correct_count = sum(1 for q in individual_questions if q.get('is_correct', False))
                        st.info(f"åŸå§‹çµæœçµ±è¨ˆï¼šç¸½å…± {total_count} å€‹å•é¡Œå›ç­”ï¼Œå…¶ä¸­ {correct_count} å€‹æ­£ç¢ºï¼Œ{total_count - correct_count} å€‹éŒ¯èª¤")
                        
                        # é¡¯ç¤ºç•¶å‰ç¯©é¸å’Œæ’åºè¨­å®šçš„å½±éŸ¿
                        st.info(f"ğŸ” ç•¶å‰è¨­å®šï¼šåˆ†æ•¸ç¯„åœ {score_filter[0]}-{score_filter[1]}%ï¼Œæ’åºï¼š{sort_by}ï¼Œæ¯é é¡¯ç¤º {items_per_page} é …")
                    
                    display_individual_questions(
                        individual_questions,
                        score_filter,
                        items_per_page,
                        sort_by,
                        is_aggregated=False,
                        section_key="individual"
                    )
            
            elif 'config' in parsed_data and 'timestamp' in parsed_data:
                st.success(f"âœ… æˆåŠŸè¼‰å…¥ Twinkle Eval çµæœæª”æ¡ˆï¼š{uploaded_files[0].name if len(uploaded_files) == 1 else 'æª”æ¡ˆ'}")
                
                # é¡¯ç¤ºè©•æ¸¬è³‡è¨Š
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                    st.metric("è©•æ¸¬æ™‚é–“", parsed_data['timestamp'])
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with col2:
                    model_name = parsed_data['config'].get('model', {}).get('name', 'N/A')
                    st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                    st.metric("æ¨¡å‹åç¨±", model_name)
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with col3:
                    duration = parsed_data.get('duration_seconds', 0)
                    st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                    st.metric("è©•æ¸¬æ™‚é•·", f"{duration:.1f}ç§’")
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with col4:
                    repeat_runs = parsed_data['config'].get('evaluation', {}).get('repeat_runs', 1)
                    st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                    st.metric("é‡è¤‡åŸ·è¡Œæ¬¡æ•¸", repeat_runs)
                    st.markdown('</div>', unsafe_allow_html=True)
                
                st.markdown("---")
                
                # é¡¯ç¤ºçµæœ
                if 'results' in parsed_data and parsed_data['results']:
                    st.header("ğŸ“Š è©•æ¸¬çµæœç¸½è¦½")
                    
                    # æº–ç¢ºç‡åœ–è¡¨
                    accuracy_chart = create_accuracy_chart(parsed_data['results'])
                    st.plotly_chart(accuracy_chart, use_container_width=True)
                    
                    # è©³ç´°çµæœ
                    st.header("ğŸ“‹ è©³ç´°çµæœ")
                    
                    for result in parsed_data['results']:
                        with st.container():
                            st.markdown('<div class="dataset-card">', unsafe_allow_html=True)
                            
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                st.metric(
                                    "è³‡æ–™é›†",
                                    result['file'].split('/')[-1],
                                    help=result['file']
                                )
                            
                            with col2:
                                accuracy_percent = result['accuracy_mean'] * 100
                                st.metric(
                                    "å¹³å‡æº–ç¢ºç‡",
                                    f"{accuracy_percent:.2f}%",
                                    delta=f"Â±{result['accuracy_std']*100:.2f}%"
                                )
                            
                            with col3:
                                if 'individual_runs' in result and 'accuracies' in result['individual_runs']:
                                    runs_count = len(result['individual_runs']['accuracies'])
                                    st.metric("åŸ·è¡Œæ¬¡æ•¸", runs_count)
                            
                            # å¤šè¼ªæ¯”è¼ƒåœ–è¡¨
                            if 'individual_runs' in result:
                                runs_chart = create_runs_comparison_chart(result)
                                st.plotly_chart(runs_chart, use_container_width=True)
                            
                            st.markdown('</div>', unsafe_allow_html=True)
                
                # è™•ç†ç©ºçµæœçš„æƒ…æ³
                elif 'dataset_results' in parsed_data:
                    st.header("ğŸ“Š è©•æ¸¬ç‹€æ…‹")
                    st.warning("âš ï¸ è©•æ¸¬æª”æ¡ˆè¼‰å…¥æˆåŠŸï¼Œä½†æ²’æœ‰æ‰¾åˆ°è©•æ¸¬çµæœ")
                    
                    # é¡¯ç¤ºè³‡æ–™é›†è³‡è¨Š
                    dataset_info = parsed_data['dataset_results']
                    
                    with st.expander("ğŸ“‹ è©³ç´°è³‡è¨Š", expanded=True):
                        for dataset_path, dataset_data in dataset_info.items():
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                st.metric("è³‡æ–™é›†è·¯å¾‘", dataset_path)
                            
                            with col2:
                                results_count = len(dataset_data.get('results', []))
                                st.metric("çµæœæ•¸é‡", results_count)
                            
                            with col3:
                                avg_accuracy = dataset_data.get('average_accuracy', 0)
                                st.metric("å¹³å‡æº–ç¢ºç‡", f"{avg_accuracy:.2f}%")
                        
                        st.info("""
                        **å¯èƒ½çš„åŸå› ï¼š**
                        - è©•æ¸¬éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤æˆ–è¢«ä¸­æ–·
                        - è³‡æ–™é›†æª”æ¡ˆæ ¼å¼ä¸æ­£ç¢º
                        - API é€£æ¥å•é¡Œå°è‡´è©•æ¸¬å¤±æ•—
                        - è©•æ¸¬å°šæœªå®Œæˆ
                        
                        **å»ºè­°æª¢æŸ¥ï¼š**
                        - æŸ¥çœ‹ logs/ è³‡æ–™å¤¾ä¸­çš„è©•æ¸¬æ—¥èªŒ
                        - ç¢ºèªè³‡æ–™é›†æª”æ¡ˆæ ¼å¼æ­£ç¢º
                        - æª¢æŸ¥ API é€£æ¥è¨­å®š
                        """)
                
                # é¡¯ç¤ºå€‹åˆ¥é¡Œç›®çµæœï¼ˆå¦‚æœæ˜¯ JSONL æ ¼å¼ï¼‰
                elif 'individual_questions' in parsed_data:
                    st.header("ğŸ“ å€‹åˆ¥é¡Œç›®çµæœ")
                    display_individual_questions(
                        parsed_data['individual_questions'],
                        score_filter,
                        items_per_page,
                        sort_by,
                        is_aggregated=False,
                        section_key="jsonl_single"
                    )
                
                else:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°å¯é¡¯ç¤ºçš„è©•æ¸¬çµæœè³‡æ–™")
            
            elif isinstance(raw_data, list):
                # è™•ç†ç´” JSONL å€‹åˆ¥é¡Œç›®æ ¼å¼
                st.success(f"âœ… æˆåŠŸè¼‰å…¥å€‹åˆ¥é¡Œç›®çµæœæª”æ¡ˆï¼š{uploaded_file.name}")
                
                # çµ±è¨ˆè³‡è¨Š
                total_questions = len(raw_data)
                correct_questions = sum(1 for q in raw_data if q.get('is_correct', False))
                accuracy = (correct_questions / total_questions) * 100 if total_questions > 0 else 0
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ç¸½é¡Œç›®æ•¸", total_questions)
                with col2:
                    st.metric("ç­”å°é¡Œç›®", correct_questions)
                with col3:
                    st.metric("æ•´é«”æº–ç¢ºç‡", f"{accuracy:.2f}%")
                
                st.markdown("---")
                
                st.header("ğŸ“ å€‹åˆ¥é¡Œç›®çµæœ")
                display_individual_questions(raw_data, score_filter, items_per_page, sort_by, is_aggregated=False, section_key="raw_jsonl")
            
            else:
                st.error("âŒ ç„¡æ³•è­˜åˆ¥çš„æª”æ¡ˆæ ¼å¼ï¼Œè«‹ç¢ºèªä¸Šå‚³çš„æ˜¯æœ‰æ•ˆçš„ Twinkle Eval çµæœæª”æ¡ˆ")
    
    else:
        # æ­¡è¿ç•«é¢
        st.markdown("""
        ## æ­¡è¿ä½¿ç”¨ Twinkle Eval Analyzer! ğŸ‘‹
        
        é€™å€‹å·¥å…·å¯ä»¥å¹«åŠ©ä½ åˆ†æ Twinkle Eval çš„è©•æ¸¬çµæœï¼Œæ”¯æ´ä»¥ä¸‹åŠŸèƒ½ï¼š
        
        - ğŸ“ **æª”æ¡ˆæ”¯æ´**ï¼šæ”¯æ´ `.json` å’Œ `.jsonl` æ ¼å¼çš„è©•æ¸¬çµæœ
        - ğŸ“Š **è¦–è¦ºåŒ–åœ–è¡¨**ï¼šæº–ç¢ºç‡åˆ†æã€å¤šè¼ªè©•æ¸¬æ¯”è¼ƒ
        - ğŸ” **è©³ç´°æª¢è¦–**ï¼šå€‹åˆ¥é¡Œç›®çµæœã€éŒ¯èª¤åˆ†æ
        - âš™ï¸ **éˆæ´»è¨­å®š**ï¼šåˆ†æ•¸éæ¿¾ã€æ’åºã€åˆ†é ç­‰åŠŸèƒ½
        
        ### é–‹å§‹ä½¿ç”¨
        è«‹åœ¨å·¦å´é‚Šæ¬„ä¸Šå‚³ä½ çš„ Twinkle Eval çµæœæª”æ¡ˆä¾†é–‹å§‹åˆ†æï¼
        
        ### æ”¯æ´æ ¼å¼
        - **results_*.json**ï¼šå®Œæ•´è©•æ¸¬çµæœæª”æ¡ˆ
        - **eval_results_*_run*.jsonl**ï¼šå€‹åˆ¥é¡Œç›®è©³ç´°çµæœæª”æ¡ˆ
        """)

if __name__ == "__main__":
    main()